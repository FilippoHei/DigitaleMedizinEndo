{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Surgical_Instrument_Segmentation_MultiInstance.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8fecb5df"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ac3264"
      },
      "source": [
        "%matplotlib inline"
      ],
      "id": "51ac3264",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44d6650d"
      },
      "source": [
        "#### Requirements\n",
        "\n",
        "\n",
        "\n",
        "1.   Connect colab to your google drive\n",
        "2.   Navigate into the folder where the unet is stored\n",
        "\n"
      ],
      "id": "44d6650d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHc0q7u-9IZ_",
        "outputId": "6dd64c95-16d3-4814-dc15-6c4936ab3ba5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "id": "GHc0q7u-9IZ_",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYAVxM5C9ja5"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/gdrive/MyDrive/unet-nested-multiple-classification')"
      ],
      "id": "yYAVxM5C9ja5",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b22edfc",
        "outputId": "a1eb948d-1cfe-440c-bde7-6a45849d4b5a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "\n",
        "# Change this path to wherever you installed the Pytorch-UNet module\n",
        "#sys.path.append('C:/Users/Groh/Documents/GitHub/unet-nested-multiple-classification')\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from unet import NestedUNet\n",
        "from unet import UNet\n",
        "from utils.dataset import BasicDataset\n",
        "from config import UNetConfig\n",
        "\n",
        "from losses import LovaszLossSoftmax\n",
        "from losses import LovaszLossHinge\n",
        "from losses import dice_coeff\n",
        "\n",
        "import cv2\n",
        "! pip install albumentations==0.4.6\n",
        "import albumentations as A\n",
        "import albumentations.augmentations.functional as F\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import copy\n",
        "\n",
        "from os.path import splitext\n",
        "from os import listdir\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "from torchvision.transforms import functional as func\n",
        "from numpy import moveaxis\n",
        "\n",
        "\n"
      ],
      "id": "3b22edfc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp37-none-any.whl size=65175 sha256=626bd6bcb6f690e3063876eac1da07ae0bda963bcd7a4f31e99f627d00fdd911\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45d353a8"
      },
      "source": [
        "#### Assing absolute paths of image and mask folders"
      ],
      "id": "45d353a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e553b737",
        "outputId": "0b59d9bc-c56f-43f0-9071-6824d1c9d855"
      },
      "source": [
        "# Change this path to the respective image and mask folders\n",
        "\n",
        "dir_img = '../gdrive/MyDrive/unet-nested-multiple-classification/data/images/'\n",
        "dir_mask = '../gdrive/MyDrive/unet-nested-multiple-classification/data/masks/'\n",
        "dir_checkpoint = '../gdrive/MyDrive/unet-nested-multiple-classification/data/checkpoints/'\n",
        "\n",
        "#print(dir_img)\n",
        "#print(dir_mask)\n",
        "#print(dir_checkpoint)\n",
        "\n",
        "images_filenames = list(sorted(os.listdir(dir_img)))\n",
        "images_filenames = images_filenames[:-1]\n",
        "print(images_filenames)\n",
        "\n",
        "#dir_img = 'C:/Users/Groh/Documents/GitHub/unet-nested-multiple-classification/data/images/'\n",
        "#print(dir_img)\n",
        "#dir_mask = 'C:/Users/Groh/Documents/GitHub/unet-nested-multiple-classification/data/masks/'\n",
        "#dir_checkpoint = 'C:/Users/Groh/Documents/GitHub/unet-nested-multiple-classification/data/checkpoints/'"
      ],
      "id": "e553b737",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aicm05_02_000017.png', 'aicm05_02_000018.png', 'aicm05_02_000021.png', 'aicm05_02_000022.png', 'aicm05_02_000023.png', 'aicm05_02_000024.png', 'aicm05_02_000025.png', 'aicm05_02_000026.png', 'aicm05_02_000027.png', 'aicm05_02_000043.png', 'aicm05_02_000044.png', 'aicm05_02_000045.png', 'aicm05_02_000046.png', 'aicm05_02_000052.png', 'aicm05_02_000053.png', 'aicm05_02_000054.png', 'aicm05_02_000055.png', 'aicm05_02_000065.png', 'aicm05_02_000066.png', 'aicm05_02_000069.png', 'aicm05_02_000070.png', 'aicm05_02_000071.png', 'aicm05_02_000072.png', 'aicm05_02_000073.png', 'aicm05_02_000094.png', 'aicm05_02_000095.png', 'aicm05_02_000096.png', 'aicm05_02_000097.png', 'aicm05_02_000098.png', 'aicm05_02_000099.png', 'aicm05_02_000109.png', 'aicm05_02_000110.png', 'aicm05_02_000112.png', 'aicm05_02_000113.png', 'aicm05_02_000114.png', 'aicm05_02_000115.png', 'aicm05_02_000116.png', 'aicm05_02_000121.png', 'aicm05_02_000122.png', 'aicm05_02_000125.png', 'aicm05_02_000126.png', 'aicm05_02_000127.png', 'aicm05_02_000128.png', 'aicm05_02_000129.png', 'aicm05_02_000130.png', 'aicm05_02_000131.png', 'aicm05_02_000132.png', 'aicm05_02_000141.png', 'aicm05_02_000142.png', 'aicm05_02_000143.png', 'aicm05_02_000144.png', 'aicm05_02_000155.png', 'aicm05_02_000157.png', 'aicm05_02_000158.png', 'aicm05_02_000159.png', 'aicm05_02_000160.png', 'aicm05_02_000161.png', 'aicm05_03_000017.png', 'aicm05_03_000018.png', 'aicm05_03_000021.png', 'aicm05_03_000022.png', 'aicm05_03_000023.png', 'aicm05_03_000024.png', 'aicm05_03_000025.png', 'aicm05_03_000026.png', 'aicm05_03_000027.png', 'aicm05_03_000043.png', 'aicm05_03_000044.png', 'aicm05_03_000045.png', 'aicm05_03_000046.png', 'aicm05_03_000052.png', 'aicm05_03_000053.png', 'aicm05_03_000054.png', 'aicm05_03_000055.png', 'aicm05_03_000065.png', 'aicm05_03_000066.png', 'aicm05_03_000070.png', 'aicm05_03_000071.png', 'aicm05_03_000072.png', 'aicm05_03_000073.png', 'aicm05_03_000094.png', 'aicm05_03_000095.png', 'aicm05_03_000096.png', 'aicm05_03_000097.png', 'aicm05_03_000098.png', 'aicm05_03_000099.png', 'aicm05_03_000109.png', 'aicm05_03_000110.png', 'aicm05_03_000112.png', 'aicm05_03_000113.png', 'aicm05_03_000114.png', 'aicm05_03_000115.png', 'aicm05_03_000116.png', 'aicm05_03_000121.png', 'aicm05_03_000122.png', 'aicm05_03_000125.png', 'aicm05_03_000126.png', 'aicm05_03_000127.png', 'aicm05_03_000128.png', 'aicm05_03_000129.png', 'aicm05_03_000130.png', 'aicm05_03_000131.png', 'aicm05_03_000132.png', 'aicm05_03_000141.png', 'aicm05_03_000142.png', 'aicm05_03_000143.png', 'aicm05_03_000144.png', 'aicm05_03_000155.png', 'aicm05_03_000157.png', 'aicm05_03_000158.png', 'aicm05_03_000159.png', 'aicm05_03_000160.png', 'aicm05_03_000161.png', 'aicm06_02_000000.png', 'aicm06_02_000005.png', 'aicm06_02_000006.png', 'aicm06_02_000009.png', 'aicm06_02_000010.png', 'aicm06_02_000012.png', 'aicm06_02_000025.png', 'aicm06_02_000026.png', 'aicm06_02_000027.png', 'aicm06_02_000028.png', 'aicm06_02_000029.png', 'aicm06_02_000030.png', 'aicm06_02_000031.png', 'aicm06_02_000034.png', 'aicm06_02_000035.png', 'aicm06_02_000041.png', 'aicm06_02_000051.png', 'aicm06_02_000052.png', 'aicm06_02_000053.png', 'aicm06_02_000054.png', 'aicm06_02_000055.png', 'aicm06_02_000056.png', 'aicm06_02_000057.png', 'aicm06_02_000066.png', 'aicm06_02_000082.png', 'aicm06_02_000090.png', 'aicm06_02_000091.png', 'aicm06_02_000092.png', 'aicm06_02_000093.png', 'aicm06_02_000094.png', 'aicm06_02_000095.png', 'aicm06_02_000096.png', 'aicm06_02_000097.png', 'aicm06_02_000098.png', 'aicm06_02_000099.png', 'aicm06_02_000100.png', 'aicm06_02_000102.png', 'aicm06_02_000103.png', 'aicm06_02_000104.png', 'aicm06_02_000105.png', 'aicm06_02_000106.png', 'aicm06_02_000107.png', 'aicm06_02_000108.png', 'aicm06_02_000109.png', 'aicm06_02_000113.png', 'aicm06_02_000114.png', 'aicm06_02_000115.png', 'aicm06_02_000127.png', 'aicm06_02_000128.png', 'aicm06_02_000129.png', 'aicm06_02_000130.png', 'aicm06_02_000131.png', 'aicm06_02_000132.png', 'aicm06_02_000133.png', 'aicm06_02_000134.png', 'aicm06_02_000135.png', 'aicm06_02_000141.png', 'aicm06_02_000142.png', 'aicm06_02_000147.png', 'aicm06_02_000149.png', 'aicm06_02_000151.png', 'aicm06_02_000167.png', 'aicm06_02_000168.png', 'aicm06_02_000169.png', 'aicm06_02_000170.png', 'aicm06_02_000171.png', 'aicm06_02_000172.png', 'aicm06_02_000173.png', 'aicm06_02_000174.png', 'aicm06_02_000175.png', 'aicm06_02_000176.png', 'aicm06_02_000177.png', 'aicm06_02_000178.png', 'aicm06_03_000000.png', 'aicm06_03_000005.png', 'aicm06_03_000006.png', 'aicm06_03_000009.png', 'aicm06_03_000010.png', 'aicm06_03_000012.png', 'aicm06_03_000025.png', 'aicm06_03_000026.png', 'aicm06_03_000027.png', 'aicm06_03_000028.png', 'aicm06_03_000029.png', 'aicm06_03_000030.png', 'aicm06_03_000031.png', 'aicm06_03_000034.png', 'aicm06_03_000035.png', 'aicm06_03_000041.png', 'aicm06_03_000051.png', 'aicm06_03_000052.png', 'aicm06_03_000053.png', 'aicm06_03_000054.png', 'aicm06_03_000055.png', 'aicm06_03_000056.png', 'aicm06_03_000057.png', 'aicm06_03_000066.png', 'aicm06_03_000082.png', 'aicm06_03_000090.png', 'aicm06_03_000091.png', 'aicm06_03_000092.png', 'aicm06_03_000093.png', 'aicm06_03_000094.png', 'aicm06_03_000095.png', 'aicm06_03_000096.png', 'aicm06_03_000097.png', 'aicm06_03_000098.png', 'aicm06_03_000099.png', 'aicm06_03_000100.png', 'aicm06_03_000102.png', 'aicm06_03_000103.png', 'aicm06_03_000104.png', 'aicm06_03_000105.png', 'aicm06_03_000106.png', 'aicm06_03_000107.png', 'aicm06_03_000108.png', 'aicm06_03_000109.png', 'aicm06_03_000113.png', 'aicm06_03_000114.png', 'aicm06_03_000115.png', 'aicm06_03_000127.png', 'aicm06_03_000128.png', 'aicm06_03_000129.png', 'aicm06_03_000130.png', 'aicm06_03_000131.png', 'aicm06_03_000132.png', 'aicm06_03_000133.png', 'aicm06_03_000134.png', 'aicm06_03_000135.png', 'aicm06_03_000141.png', 'aicm06_03_000142.png', 'aicm06_03_000147.png', 'aicm06_03_000149.png', 'aicm06_03_000151.png', 'aicm06_03_000167.png', 'aicm06_03_000168.png', 'aicm06_03_000169.png', 'aicm06_03_000170.png', 'aicm06_03_000171.png', 'aicm06_03_000172.png', 'aicm06_03_000173.png', 'aicm06_03_000174.png', 'aicm06_03_000175.png', 'aicm06_03_000176.png', 'aicm06_03_000177.png', 'aicm06_03_000178.png', 'aicm07_02_000005.png', 'aicm07_02_000006.png', 'aicm07_02_000007.png', 'aicm07_02_000008.png', 'aicm07_02_000009.png', 'aicm07_02_000010.png', 'aicm07_02_000011.png', 'aicm07_02_000012.png', 'aicm07_02_000013.png', 'aicm07_02_000014.png', 'aicm07_02_000015.png', 'aicm07_02_000016.png', 'aicm07_02_000026.png', 'aicm07_02_000027.png', 'aicm07_02_000028.png', 'aicm07_02_000029.png', 'aicm07_02_000031.png', 'aicm07_02_000032.png', 'aicm07_02_000033.png', 'aicm07_02_000034.png', 'aicm07_02_000035.png', 'aicm07_02_000036.png', 'aicm07_02_000046.png', 'aicm07_02_000052.png', 'aicm07_02_000056.png', 'aicm07_02_000075.png', 'aicm07_02_000076.png', 'aicm07_02_000081.png', 'aicm07_02_000082.png', 'aicm07_02_000083.png', 'aicm07_02_000084.png', 'aicm07_02_000091.png', 'aicm07_02_000092.png', 'aicm07_02_000093.png', 'aicm07_02_000100.png', 'aicm07_02_000106.png', 'aicm07_02_000109.png', 'aicm07_02_000120.png', 'aicm07_02_000121.png', 'aicm07_02_000122.png', 'aicm07_02_000123.png', 'aicm07_02_000124.png', 'aicm07_02_000125.png', 'aicm07_02_000126.png', 'aicm07_02_000127.png', 'aicm07_02_000128.png', 'aicm07_02_000129.png', 'aicm07_02_000130.png', 'aicm07_02_000131.png', 'aicm07_02_000160.png', 'aicm07_02_000175.png', 'aicm07_02_000177.png', 'aicm07_02_000178.png', 'aicm07_02_000180.png', 'aicm07_02_000181.png', 'aicm07_03_000005.png', 'aicm07_03_000006.png', 'aicm07_03_000007.png', 'aicm07_03_000008.png', 'aicm07_03_000009.png', 'aicm07_03_000010.png', 'aicm07_03_000011.png', 'aicm07_03_000012.png', 'aicm07_03_000013.png', 'aicm07_03_000014.png', 'aicm07_03_000015.png', 'aicm07_03_000016.png', 'aicm07_03_000026.png', 'aicm07_03_000027.png', 'aicm07_03_000028.png', 'aicm07_03_000029.png', 'aicm07_03_000031.png', 'aicm07_03_000032.png', 'aicm07_03_000033.png', 'aicm07_03_000034.png', 'aicm07_03_000035.png', 'aicm07_03_000036.png', 'aicm07_03_000046.png', 'aicm07_03_000052.png', 'aicm07_03_000056.png', 'aicm07_03_000075.png', 'aicm07_03_000076.png', 'aicm07_03_000081.png', 'aicm07_03_000082.png', 'aicm07_03_000083.png', 'aicm07_03_000084.png', 'aicm07_03_000091.png', 'aicm07_03_000092.png', 'aicm07_03_000093.png', 'aicm07_03_000100.png', 'aicm07_03_000106.png', 'aicm07_03_000109.png', 'aicm07_03_000119.png', 'aicm07_03_000120.png', 'aicm07_03_000121.png', 'aicm07_03_000122.png', 'aicm07_03_000123.png', 'aicm07_03_000124.png', 'aicm07_03_000125.png', 'aicm07_03_000127.png', 'aicm07_03_000128.png', 'aicm07_03_000129.png', 'aicm07_03_000130.png', 'aicm07_03_000131.png', 'aicm07_03_000160.png', 'aicm07_03_000178.png', 'aicm07_03_000181.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52a828d"
      },
      "source": [
        "#### Prepare masks for multi instance segmentation"
      ],
      "id": "f52a828d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e24db289"
      },
      "source": [
        "convert_files = True"
      ],
      "id": "e24db289",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ca8a4e9"
      },
      "source": [
        "num_files = len([f for f in os.listdir(dir_img)if os.path.isfile(os.path.join(dir_img, f))])\n",
        "\n",
        "if convert_files:\n",
        "    for i in range(num_files):\n",
        "        name = os.listdir(dir_img)[i]\n",
        "        file = dir_mask+name\n",
        "\n",
        "        # If a mask is missing, create a new, empty mask\n",
        "        if not os.path.isfile(file):\n",
        "            img = Image.open(dir_img+name)\n",
        "            width, height = img.size\n",
        "\n",
        "            img_new = Image.new('L', (width, height))\n",
        "            img_new.save(file, \"PNG\")\n",
        "\n",
        "        # Convert all images to 8-bit gray scales\n",
        "        img_grayscale = Image.open(file).convert('L')\n",
        "        img_grayscale.save(file)\n",
        "\n",
        "\n",
        "        # Check, whether more than background is visible in the mask\n",
        "        image_gray = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
        "        unique = np.unique(image_gray.flatten())\n",
        "        if len(unique) > 1:\n",
        "            # For every pixel change gray-scale value according to categorical value\n",
        "            img = Image.open(file)\n",
        "            # Create pixel map\n",
        "            pixels = img.load()\n",
        "            for ii in range(img.size[0]): \n",
        "                for j in range(img.size[1]):\n",
        "                    if pixels[ii,j] == 52: # Atraum. Pinzette\n",
        "                        pixels[ii,j] = 1\n",
        "                    elif pixels[ii,j] == 113: # Nadelhalter\n",
        "                        pixels[ii,j] = 2\n",
        "    #                     elif pixels[i,j] = 52:\n",
        "    #                         pixels[i,j] = 1\n",
        "    #                     elif pixels[i,j] = 52:\n",
        "    #                         pixels[i,j] = 1\n",
        "    #                     elif pixels[i,j] = 52:\n",
        "    #                         pixels[i,j] = 1\n",
        "    #                     elif pixels[i,j] = 52:\n",
        "    #                         pixels[i,j] = 1\n",
        "    #                     elif pixels[i,j] = 52:\n",
        "    #                         pixels[i,j] = 1\n",
        "            img.save(file)"
      ],
      "id": "7ca8a4e9",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "60fb49dc"
      },
      "source": [
        "for i in range(num_files):\n",
        "    name = os.listdir(dir_img)[i]\n",
        "    file = dir_mask+name\n",
        "   \n",
        "    \n",
        "    image_gray = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
        "    unique = np.unique(image_gray.flatten())\n",
        "    \n",
        "    img = Image.open(file)\n",
        "    img_array = np.array(img)\n",
        "    pictype = img_array.dtype\n",
        "    \n",
        "    picchannels = img_array.shape\n",
        "    \n",
        "    print(len(unique), unique, pictype, picchannels)"
      ],
      "id": "60fb49dc"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "cfdc243b"
      },
      "source": [
        "img = Image.open(dir_img+name)\n",
        "\n",
        "img_array = np.array(img)\n",
        "\n",
        "pictype = img_array.dtype\n",
        "\n",
        "picchannels = img_array.shape\n",
        "\n",
        "print(pictype, picchannels)"
      ],
      "id": "cfdc243b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4mpz0YHdwaQ"
      },
      "source": [
        "## Data Augmentation"
      ],
      "id": "m4mpz0YHdwaQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0D7MgNwdv98"
      },
      "source": [
        "#erstellt Dataset mit Trainingsbildern und -masken\n",
        "class TrainingDataset(Dataset):\n",
        "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.images_directory = images_directory\n",
        "        self.masks_directory = masks_directory\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.images_filenames[idx]\n",
        "        pathImage = os.path.join(self.images_directory, image_filename)\n",
        "        pathMask = os.path.join(self.masks_directory, image_filename)\n",
        "        #print(pathImage)\n",
        "        pillow_image = Image.open(pathImage)\n",
        "        image = np.array(pillow_image)\n",
        "        pillow_mask = Image.open(pathMask)\n",
        "        mask = np.array(pillow_mask)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "          transformed = self.transform(image=image, mask=mask)\n",
        "          image = transformed[\"image\"]\n",
        "          mask = transformed[\"mask\"]\n",
        "          image_np = np.array(image)\n",
        "          mask_np = np.array(mask)\n",
        "         \n",
        "          #to get channels_first format\n",
        "          image_np = moveaxis(image_np, 2,0)\n",
        "                    \n",
        "          image_tensor = torch.from_numpy(image_np).type(torch.FloatTensor)\n",
        "          mask_tensor = torch.from_numpy(mask_np).type(torch.FloatTensor)\n",
        "\n",
        "        return {\n",
        "            'image': image_tensor,\n",
        "            'mask':  mask_tensor\n",
        "        }          \n",
        "        \n",
        "#tatsächliche Augmentation\n",
        "#möglicherweise noch andere Transformationen einfügen\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate (limit=[-10, 10]),\n",
        "    A.Blur (blur_limit=4, always_apply=False, p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
        "    #A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
        "    #A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "])\n",
        "\n",
        "train_dataset = TrainingDataset(images_filenames, dir_img, dir_mask, train_transform)\n"
      ],
      "id": "I0D7MgNwdv98",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4aYfiv5qpk0"
      },
      "source": [
        "Visualization of different augmentations applied to the same image and the associated mask"
      ],
      "id": "N4aYfiv5qpk0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPVbDpOq8GY"
      },
      "source": [
        "#samples: defines the number of augmentations to display\n",
        "def visualize_augmentations(dataset, idx=0, samples=15):\n",
        "\n",
        "    dataset = copy.deepcopy(dataset)\n",
        "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
        "    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n",
        "    for i in range(samples):\n",
        "        image, mask = dataset[idx]\n",
        "        #image = np.array(image)\n",
        "        #mask = np.array(mask)\n",
        "        ax[i, 0].imshow(image)\n",
        "        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n",
        "        ax[i, 0].set_title(\"Augmented image\")\n",
        "        ax[i, 1].set_title(\"Augmented mask\")\n",
        "        ax[i, 0].set_axis_off()\n",
        "        ax[i, 1].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return test"
      ],
      "id": "ZLPVbDpOq8GY",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1ne-cKOrxY2"
      },
      "source": [
        "#random.seed(42)\n",
        "#test = visualize_augmentations(train_dataset, idx=2)"
      ],
      "id": "Q1ne-cKOrxY2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdvzQRAM8ujM"
      },
      "source": [
        ""
      ],
      "id": "CdvzQRAM8ujM",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a61298"
      },
      "source": [
        "#### Train network"
      ],
      "id": "96a61298"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5523fc5c",
        "scrolled": true
      },
      "source": [
        "# Change UNet configuration if necessary\n",
        "# Also important to change n_channels and n_classes\n",
        "cfg = UNetConfig()\n",
        "\n",
        "# Training function\n",
        "def train_net(net, cfg):\n",
        "    val_percent = cfg.validation / 100\n",
        "    n_val = int(len(train_dataset) * val_percent)\n",
        "    n_train = len(train_dataset) - n_val\n",
        "       \n",
        "    train, val = random_split(train_dataset, [n_train, n_val])\n",
        "   \n",
        "    train_loader = DataLoader(train,\n",
        "                              batch_size=cfg.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              pin_memory=True)\n",
        "    val_loader = DataLoader(val,\n",
        "                            batch_size=cfg.batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=8,\n",
        "                            pin_memory=True)\n",
        "\n",
        "    writer = SummaryWriter(comment=f'LR_{cfg.lr}_BS_{cfg.batch_size}_SCALE_{cfg.scale}')\n",
        "    global_step = 0\n",
        "\n",
        "    logging.info(f'''Starting training:\n",
        "        Epochs:          {cfg.epochs}\n",
        "        Batch size:      {cfg.batch_size}\n",
        "        Learning rate:   {cfg.lr}\n",
        "        Optimizer:       {cfg.optimizer}\n",
        "        Training size:   {n_train}\n",
        "        Validation size: {n_val}\n",
        "        Checkpoints:     {cfg.save_cp}\n",
        "        Device:          {device.type}\n",
        "        Images scaling:  {cfg.scale}\n",
        "    ''')\n",
        "\n",
        "    if cfg.optimizer == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(),\n",
        "                               lr=cfg.lr)\n",
        "    elif cfg.optimizer == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(),\n",
        "                                  lr=cfg.lr,\n",
        "                                  weight_decay=cfg.weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.SGD(net.parameters(),\n",
        "                              lr=cfg.lr,\n",
        "                              momentum=cfg.momentum,\n",
        "                              weight_decay=cfg.weight_decay,\n",
        "                              nesterov=cfg.nesterov)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                               milestones=cfg.lr_decay_milestones,\n",
        "                                               gamma = cfg.lr_decay_gamma)\n",
        "    if cfg.n_classes > 1:\n",
        "        criterion = LovaszLossSoftmax()\n",
        "    else:\n",
        "        criterion = LovaszLossHinge()\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        net.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{cfg.epochs}', unit='img') as pbar:\n",
        "            for batch in train_loader:\n",
        "                batch_imgs = batch['image']\n",
        "                batch_masks = batch['mask']\n",
        "                assert batch_imgs.shape[1] == cfg.n_channels, \\\n",
        "                        f'Network has been defined with {cfg.n_channels} input channels, ' \\\n",
        "                        f'but loaded images have {batch_imgs.shape[1]} channels. Please check that ' \\\n",
        "                        'the images are loaded correctly.'\n",
        "\n",
        "                batch_imgs = batch_imgs.to(device=device, dtype=torch.float32)\n",
        "                mask_type = torch.float32 if cfg.n_classes == 1 else torch.long\n",
        "                batch_masks = batch_masks.to(device=device, dtype=mask_type)\n",
        "\n",
        "                inference_masks = net(batch_imgs)\n",
        "\n",
        "                if cfg.n_classes == 1:\n",
        "                    inferences = inference_masks.squeeze(1)\n",
        "                    masks = batch_masks.squeeze(1)\n",
        "                else:\n",
        "                    inferences = inference_masks\n",
        "                    masks = batch_masks\n",
        "\n",
        "                if cfg.deepsupervision:\n",
        "                    loss = 0\n",
        "                    for inference_mask in inferences:\n",
        "                        loss += criterion(inference_mask, masks)\n",
        "                    loss /= len(inferences)\n",
        "                else:\n",
        "                    loss = criterion(inferences, masks)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
        "                writer.add_scalar('model/lr', optimizer.param_groups[0]['lr'], global_step)\n",
        "\n",
        "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                pbar.update(batch_imgs.shape[0])\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % (len(train_dataset) // (1 * cfg.batch_size)) == 0:\n",
        "                    val_score = eval_net(net, val_loader, device, n_val, cfg)\n",
        "                    if cfg.n_classes > 1:\n",
        "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
        "                        writer.add_scalar('CrossEntropy/test', val_score, global_step)\n",
        "                    else:\n",
        "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
        "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
        "\n",
        "                    writer.add_images('images', batch_imgs, global_step)\n",
        "                    if cfg.deepsupervision:\n",
        "                            inference_masks = inference_masks[-1]\n",
        "                    if cfg.n_classes == 1:\n",
        "                        # writer.add_images('masks/true', batch_masks, global_step)\n",
        "                        inference_mask = torch.sigmoid(inference_masks) > cfg.out_threshold\n",
        "                        writer.add_images('masks/inference',\n",
        "                                          inference_mask,\n",
        "                                          global_step)\n",
        "                    else:\n",
        "                        # writer.add_images('masks/true', batch_masks, global_step)\n",
        "                        ids = inference_masks.shape[1]  # N x C x H x W\n",
        "                        inference_masks = torch.chunk(inference_masks, ids, dim=1)\n",
        "                        for idx in range(0, len(inference_masks)):\n",
        "                            inference_mask = torch.sigmoid(inference_masks[idx]) > cfg.out_threshold\n",
        "                            writer.add_images('masks/inference_'+str(idx),\n",
        "                                              inference_mask,\n",
        "                                              global_step)\n",
        "\n",
        "        if cfg.save_cp:\n",
        "            try:\n",
        "                os.mkdir(cfg.checkpoints_dir)\n",
        "                logging.info('Created checkpoint directory')\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "            ckpt_name = 'epoch_' + str(epoch + 1) + '.pth'\n",
        "            torch.save(net.state_dict(),\n",
        "                       osp.join(cfg.checkpoints_dir, ckpt_name))\n",
        "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
        "\n",
        "    writer.close()\n",
        "    \n",
        "\n",
        "# Evaluation function\n",
        "def eval_net(net, loader, device, n_val, cfg):\n",
        "    \"\"\"\n",
        "    Evaluation without the densecrf with the dice coefficient\n",
        "\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    tot = 0\n",
        "\n",
        "    with tqdm(total=n_val, desc='Validation round', unit='img', leave=False) as pbar:\n",
        "        for batch in loader:\n",
        "            imgs = batch['image']\n",
        "            true_masks = batch['mask']\n",
        "\n",
        "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "            mask_type = torch.float32 if cfg.n_classes == 1 else torch.long\n",
        "            true_masks = true_masks.to(device=device, dtype=mask_type)\n",
        "\n",
        "            # compute loss\n",
        "            if cfg.deepsupervision:\n",
        "                masks_preds = net(imgs)\n",
        "                loss = 0\n",
        "                for masks_pred in masks_preds:\n",
        "                    tot_cross_entropy = 0\n",
        "                    for true_mask, pred in zip(true_masks, masks_pred):\n",
        "                        pred = (pred > cfg.out_threshold).float()\n",
        "                        if cfg.n_classes > 1:\n",
        "                            sub_cross_entropy = F.cross_entropy(pred.unsqueeze(dim=0), true_mask.unsqueeze(dim=0).squeeze(1)).item()\n",
        "                        else:\n",
        "                            sub_cross_entropy = dice_coeff(pred, true_mask.squeeze(dim=1)).item()\n",
        "                        tot_cross_entropy += sub_cross_entropy\n",
        "                    tot_cross_entropy = tot_cross_entropy / len(masks_preds)\n",
        "                    tot += tot_cross_entropy\n",
        "            else:\n",
        "                masks_pred = net(imgs)\n",
        "                for true_mask, pred in zip(true_masks, masks_pred):\n",
        "                    pred = (pred > cfg.out_threshold).float()\n",
        "                    if cfg.n_classes > 1:\n",
        "                        tot += F.cross_entropy(pred.unsqueeze(dim=0), true_mask.unsqueeze(dim=0).squeeze(1)).item()\n",
        "                    else:\n",
        "                        tot += dice_coeff(pred, true_mask.squeeze(dim=1)).item()\n",
        "\n",
        "            pbar.update(imgs.shape[0])\n",
        "\n",
        "    return tot / n_val\n"
      ],
      "id": "5523fc5c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fecb5df"
      },
      "source": [
        "#### Configure training parameters and device"
      ],
      "id": "8fecb5df"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "296d24b4",
        "outputId": "b0becc42-d23d-4b3f-871e-0cb17fdc025f"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Automatically uses a GPU, if it is available to torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "logging.info(f'Using device {device}')\n",
        "\n",
        "net = eval(cfg.model)(cfg)\n",
        "logging.info(f'Network:\\n'\n",
        "             f'\\t{cfg.model} model\\n'\n",
        "             f'\\t{cfg.n_channels} input channels\\n'\n",
        "             f'\\t{cfg.n_classes} output channels (classes)\\n'\n",
        "             f'\\t{\"Bilinear\" if cfg.bilinear else \"Dilated conv\"} upscaling')\n",
        "\n",
        "if cfg.load:\n",
        "    net.load_state_dict(\n",
        "        torch.load(cfg.load, map_location=device)\n",
        "    )\n",
        "    logging.info(f'Model loaded from {cfg.load}')\n",
        "\n",
        "net.to(device=device);\n",
        "# faster convolutions, but more memory\n",
        "# cudnn.benchmark = True"
      ],
      "id": "296d24b4",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Using device cpu\n",
            "INFO: Network:\n",
            "\tNestedUNet model\n",
            "\t3 input channels\n",
            "\t3 output channels (classes)\n",
            "\tBilinear upscaling\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dbf6ba"
      },
      "source": [
        "#### Call the training function"
      ],
      "id": "96dbf6ba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c82f2e2",
        "scrolled": true,
        "outputId": "79b0979e-b8e2-43a8-8d25-087e2d55376b"
      },
      "source": [
        "try:\n",
        "    train_net(net=net, cfg=cfg)\n",
        "except KeyboardInterrupt:\n",
        "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
        "    logging.info('Saved interrupt')\n",
        "    try:\n",
        "        sys.exit(0)\n",
        "    except SystemExit:\n",
        "        os._exit(0)"
      ],
      "id": "7c82f2e2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "INFO: Starting training:\n",
            "        Epochs:          2\n",
            "        Batch size:      2\n",
            "        Learning rate:   0.0001\n",
            "        Optimizer:       SGD\n",
            "        Training size:   330\n",
            "        Validation size: 36\n",
            "        Checkpoints:     True\n",
            "        Device:          cpu\n",
            "        Images scaling:  1\n",
            "    \n",
            "Epoch 1/2:   0%|          | 0/330 [00:00<?, ?img/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch 1/2:  73%|███████▎  | 242/330 [1:44:23<38:23, 26.18s/img, loss (batch)=0.675]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}